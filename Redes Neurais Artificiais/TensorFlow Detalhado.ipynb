{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow Detalhado\n",
    "\n",
    "No último tutorial, mostrei o essencial para montar uma rede neural no TensorFlow. Aqui, vamos entrar em alguns detalhes que podem ajudar a tornar o código mais organizado e fácil de entender. Vamos considerar como exemplo rede neural para classificar imagens de dígitos escritos a mão. As imagens pertencem a 10 classes, do dígito 0 ao 9. Os dados são imagens de 28 por 28 pixeis, o que nos dá 784 variáveis. Nos tutoriais passados, nossos alvos eram vetores *one-hot*. Agora, eles serão simples variáveis categóricas, com a categoria 0 representando o dígito zero,  a categoria 1 representando o dígito 1 e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAFPdJREFUeJzt3X+M3HWdx/HnW0HAGrbhV8sPf5Sf1lxEdz08TopcMOkd\nJqAi4ggR9Y+GoMSs8SQaFU6DP1Cop9iLBFAJMAnqGTmDrWBKPVTg0j09wGIjV6kKrYWareFHgfZz\nf8z0nF3K7nd257vv3dnnI5nE+c57Zt4fv8urn/nO5/udKKUgScrxouwGJGk+M4QlKZEhLEmJDGFJ\nSmQIS1IiQ1iSEhnCkpTIEJakRIawJCXaJ7uBiDgYWA78Dng6txtJ6on9gVcBa0opj09UWFsIR8QH\ngY8Ci4FfAReXUv5rL6XLgZvq6kOSEp0H3DxRQS0hHBHnAlcCK4B7gWFgTUQcX0p5bFz57wBuvPFG\nli5dOuaB4eFhVq5cWUeL6Rzb3NXP4+vnscHMjW/Dhg2cf/750M63idQ1Ex4GvlFKuQEgIi4E3gp8\nALhiXO3TAEuXLmVwcHDMAwMDA8/b1i8c29zVz+Pr57FByvgmPcTa8y/mImJfYAj4yZ5tpXWptjuA\nk3v9fpI0l9WxOuIQ4MXA1nHbt9I6PixJanOJmiQlquOY8GPALmDRuO2LgC0v9KTh4WEGBgbGbHvl\nK1/Z8+Zmi0ajkd1Cbfp5bNDf4+vnsUE942s2mzSbzTHbRkdHKz8/6vhljYi4G7inlPLh9v0ANgNf\nLaV8aVztILB+/fr1ff2FgKT5Y2RkhKGhIYChUsrIRLV1rY64CvhWRKznr0vUXgp8q6b3k6Q5qZYQ\nLqXcEhGHAJ+hdRjil8DyUsq2Ot5Pkuaq2s6YK6WsAlbV9fqS1A9cHSFJiQxhSUpkCEtSIkNYkhIZ\nwpKUyBCWpESGsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iSEhnCkpTIEJakRIawJCUyhCUpkSEsSYkM\nYUlKZAhLUiJDWJISGcKSlMgQlqREhrAkJTKEJSmRISxJiQxhSUpkCEtSIkNYkhIZwpKUyBCWpESG\nsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iSEhnCkpTIEJakRIawJCUyhCUpkSEsSYkMYUlKZAhLUiJD\nWJIS9TyEI+LSiNg97vbrXr+PJPWDfWp63fuB04Fo33+upveRpDmtrhB+rpSyrabXlqS+Udcx4eMi\n4o8R8VBE3BgRL6/pfSRpTqsjhO8G3gcsBy4ElgA/jYgFNbyXJM1pPT8cUUpZ03H3/oi4F3gYeBfw\nzV6/nyTNZXUdE/5/pZTRiNgIHDtR3fDwMAMDA2O2NRoNGo1Gne1J0rQ0m02azeaYbaOjo5WfH6WU\nXvc09g0iXkZrJnxpKeXqvTw+CKxfv349g4ODtfYiSTNhZGSEoaEhgKFSyshEtXWsE/5SRJwaEa+M\niL8Hvk9riVpzkqdK0rxTx+GIo4CbgYOBbcBdwN+VUh6v4b0kaU6r44s5D+JKUkVeO0KSEhnCkpTI\nEJakRIawJCUyhCUpkSEsSYkMYUlKZAhLUiJDWJISGcKSlMgQlqREtV9PWLPDjh07Ktc+/vjcutbS\npk2bKtcuWbKktj6uv/76yrVXXnll5dqLL764cu3ChQsr137kIx+pXLvffvtVrlV3nAlLUiJDWJIS\nGcKSlMgQlqREhrAkJTKEJSmRISxJiQxhSUpkCEtSIkNYkhJ52vIM2L17d+XaH/7wh5VrL7/88sq1\nmzdvrly7ZcuWyrX6qxe9qPqcZv/9969ce8UVV0ylnUnt3Lmzcu1ll11WSw9yJixJqQxhSUpkCEtS\nIkNYkhIZwpKUyBCWpESGsCQlMoQlKZEhLEmJDGFJSuRpyzOgm1ORzzrrrFp66OY02bPPPruWHk44\n4YTKtcuXL69ce9NNN1Wu7faXpE888cTKteecc07l2mOOOaZy7Ze//OXKtZ/4xCcq127fvr1yrerj\nTFiSEhnCkpTIEJakRIawJCUyhCUpkSEsSYkMYUlKZAhLUiJDWJISGcKSlMjTlmfAKaecUrn2jjvu\nqFx75JFHVq49/PDDK9cODAxUrp0NTj311OwWuvbMM89Urr399ttr6eGCCy6o5XXVna5nwhGxLCJu\njYg/RsTuiDhzLzWfiYhHIuLJiLg9Io7tTbuS1F+mcjhiAfBL4CKgjH8wIi4BPgSsAE4CngDWRMRL\nptGnJPWlrg9HlFJWA6sBIiL2UvJh4LOllB+2a94LbAXeBtwy9VYlqf/09Iu5iFgCLAZ+smdbKWUH\ncA9wci/fS5L6Qa9XRyymdYhi67jtW9uPSZI6uERNkhL1eonaFiCARYydDS8C/nuiJw4PDz9vaVSj\n0aDRaPS4RUnqnWazSbPZHLNtdHS08vN7GsKllE0RsQU4HfgfgIg4EHgj8PWJnrty5UoGBwd72Y4k\n1W5vk8WRkRGGhoYqPb/rEI6IBcCxtGa8AEdHxInA9lLK74GvAJ+MiN8CvwM+C/wB+EG37yVJ/W4q\nM+E3AGtpfQFXgCvb278NfKCUckVEvBT4BrAQ+E/gn0op1U8RkqR5YirrhNcxyRd6pZTLgMum1lL/\nOeiggyrXnn766TV2otli586dlWvXrl1buXbBggWVa7s5lV31cXWEJCUyhCUpkSEsSYkMYUlKZAhL\nUiJDWJISGcKSlMgQlqREhrAkJTKEJSmRv7Ys9cizzz5bufajH/1oLT2sW7eucu0RRxxRSw/qjjNh\nSUpkCEtSIkNYkhIZwpKUyBCWpESGsCQlMoQlKZEhLEmJDGFJSmQIS1IiT1uWeuTBBx+sXHvNNdfU\n0sPSpUtreV3Vx5mwJCUyhCUpkSEsSYkMYUlKZAhLUiJDWJISGcKSlMgQlqREhrAkJTKEJSmRISxJ\nibx2hNQjO3bsqOV1u7nOxAEHHFBLD6qPM2FJSmQIS1IiQ1iSEhnCkpTIEJakRIawJCUyhCUpkSEs\nSYkMYUlKZAhLUqKuT1uOiGXAPwNDwOHA20opt3Y8/k3ggnFPW11KOWM6jUoZdu3aVbn2U5/6VOXa\nww47rHLt+eefX7k2IirXanaYykx4AfBL4CKgvEDNj4BFwOL2rTGl7iSpz3U9Ey6lrAZWA8QL/7O7\ns5SybTqNSdJ8UNcx4dMiYmtEPBgRqyLioJreR5LmtDouZfkj4HvAJuAY4PPAbRFxcinlhQ5fSNK8\n1PMQLqXc0nH3gYi4D3gIOA1Y2+v3k6S5rPaLupdSNkXEY8CxTBDCw8PDDAwMjNnWaDRoNPxOT9Ls\n1Ww2aTabY7aNjo5Wfn7tIRwRRwEHA49OVLdy5UoGBwfrbkeSempvk8WRkRGGhoYqPX8q64QX0JrV\n7lkZcXREnAhsb98upXVMeEu77ovARmBNt+8lSf1uKjPhN9A6rFDatyvb279Na+3wa4H3AguBR2iF\n76dLKc9Ou1tJ6jNTWSe8jomXtv3j1NuRpPnFX1uWJnDLLbdMXtS2dm31xT8rVqyoXOsvKPc3L+Aj\nSYkMYUlKZAhLUiJDWJISGcKSlMgQlqREhrAkJTKEJSmRISxJiQxhSUrkacuaV7r9cZcbbrihcm03\nv3T86U9/uqs+1L+cCUtSIkNYkhIZwpKUyBCWpESGsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iSEnna\nsuaVa6+9tqv61atXV65997vfXbn2yCOP7KoP9S9nwpKUyBCWpESGsCQlMoQlKZEhLEmJDGFJSmQI\nS1IiQ1iSEhnCkpTIEJakRJ62rHllw4YNtb326173utpeW/3LmbAkJTKEJSmRISxJiQxhSUpkCEtS\nIkNYkhIZwpKUyBCWpESGsCQlMoQlKVFXpy1HxMeBtwOvBp4Cfg5cUkrZ2FGzH3AVcC6wH7AGuKiU\n8qdeNS112r17d+XaW2+9tavX3mef6v+JvOMd7+jqtSXofia8DPga8EbgLcC+wI8j4oCOmq8AbwXO\nBk4FjgC+N/1WJan/dDUTLqWc0Xk/It4H/AkYAu6KiAOBDwDvLqWsa9e8H9gQESeVUu7tSdeS1Cem\ne0x4IVCA7e37Q7SC/Sd7CkopvwE2AydP870kqe9MOYQjImgderirlPLr9ubFwDOllB3jyre2H5Mk\ndZjO9YRXAa8BTulRL5I070wphCPiauAMYFkp5ZGOh7YAL4mIA8fNhhe1H3tBw8PDDAwMjNnWaDRo\nNBpTaVGSZkSz2aTZbI7ZNjo6Wvn5XYdwO4DPAt5cStk87uH1wHPA6cD32/UnAK8AfjHR665cuZLB\nwcFu25GkVHubLI6MjDA0NFTp+d2uE14FNIAzgSciYlH7odFSytOllB0RcR1wVUT8GfgL8FXgZ66M\nkKTn63YmfCGt1RB3jtv+fuCG9v8eBnYB36V1ssZq4INTb1GS+le364QnXU1RStkJXNy+SZIm4K8t\na87buHHj5EVtDz30UFev3c2pyMcdd1xXry2BF/CRpFSGsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iS\nEhnCkpTIEJakRIawJCXytGXNeStWrKjttc8777zaXlsCZ8KSlMoQlqREhrAkJTKEJSmRISxJiQxh\nSUpkCEtSIkNYkhIZwpKUyBCWpESetqxZ6bnnnqtc++STT9bWx7Jly2p7bQmcCUtSKkNYkhIZwpKU\nyBCWpESGsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iSEhnCkpTIa0doVnr00Ucr165fv762Ph5++OHK\ntYceemhtfah/OROWpESGsCQlMoQlKZEhLEmJDGFJSmQIS1IiQ1iSEhnCkpTIEJakRIawJCXq6rTl\niPg48Hbg1cBTwM+BS0opGztq7gRO7XhaAb5RSrlo2t1q3rjuuutqed0vfOELXdUPDQ3V0oe0R7cz\n4WXA14A3Am8B9gV+HBEHdNQU4BpgEbAYOBz42PRblaT+09VMuJRyRuf9iHgf8CdgCLir46EnSynb\npt2dJPW56R4TXkhr5rt93PbzImJbRNwXEZ8bN1OWJLVN+VKWERHAV4C7Sim/7njoJuBh4BHgtcAV\nwPHAO6fRpyT1pelcT3gV8BrgTZ0bSynXdtx9ICK2AHdExJJSyqZpvJ8k9Z0phXBEXA2cASwrpUx2\n9e17gACOBV4whIeHhxkYGBizrdFo0Gg0ptKiJM2IZrNJs9kcs210dLTy87sO4XYAnwW8uZSyucJT\nXk/ruPGEYb1y5UoGBwe7bUeSUu1tsjgyMlJ5eWO364RXAQ3gTOCJiFjUfmi0lPJ0RBwNvAe4DXgc\nOBG4ClhXSrm/m/eSpPmg25nwhbRmtXeO2/5+4AbgGVrrhz8MLAB+D3wHuHxaXUpSn+p2nfCES9pK\nKX8ATptOQ5I0n/hry5qVli5dWrm2tVqymnPOOaerPrp5bWkqvICPJCUyhCUpkSEsSYkMYUlKZAhL\nUiJDWJISGcKSlMgQlqREhrAkJTKEJSmRpy1rVjr33HNrqZVmG2fCkpTIEJakRIawJCUyhCUp0awO\n4fE/ntdPHNvc1c/j6+exwewcnyGcxLHNXf08vn4eG8zO8c3qEJakfmcIS1IiQ1iSEs2GM+b2B9iw\nYcPzHhgdHWVkZGTGG5oJjm3u6ufx9fPYYObG15Fn+09WG6WUeruZrIGI9wA3pTYhSfU4r5Ry80QF\nsyGEDwaWA78Dnk5tRpJ6Y3/gVcCaUsrjExWmh7AkzWd+MSdJiQxhSUpkCEtSIkNYkhLNyhCOiA9G\nxKaIeCoi7o6Iv83uqRci4tKI2D3u9uvsvqYiIpZFxK0R8cf2OM7cS81nIuKRiHgyIm6PiGMzep2K\nycYXEd/cy768LavfqiLi4xFxb0TsiIitEfH9iDh+XM1+EfH1iHgsIv4SEd+NiMOyeu5GxfHdOW6/\n7YqIVVk9z7oQjohzgSuBS4HXA78C1kTEIamN9c79wCJgcft2Sm47U7YA+CVwEfC8JTYRcQnwIWAF\ncBLwBK39+JKZbHIaJhxf248Yuy8bM9PatCwDvga8EXgLsC/w44g4oKPmK8BbgbOBU4EjgO/NcJ9T\nVWV8BbiGv+67w4GPzXCfHd2UMqtuwN3Av3bcD+APwMeye+vB2C4FRrL7qGFcu4Ezx217BBjuuH8g\n8BTwrux+ezS+bwL/nt1bD8Z2SHt8p3Tsp53A2ztqTmjXnJTd73TH1962Frgqu7c9t1k1E46IfYEh\n4Cd7tpXW/2t3ACdn9dVjx7U/4j4UETdGxMuzG+q1iFhCa4bRuR93APfQP/sR4LT2R94HI2JVRByU\n3dAULKQ1M9zevj9E63IGnfvuN8Bm5ua+Gz++Pc6LiG0RcV9EfG7cTHlGzYZrR3Q6BHgxsHXc9q20\n/jWe6+4G3gf8htZHoMuAn0bE35RSnkjsq9cW0/rD39t+XDzz7dTiR7Q+om8CjgE+D9wWESe3Jw6z\nXkQErUMPd5VS9nw3sRh4pv2PZqc5t+9eYHzQukzCw7Q+rb0WuAI4HnjnjDfJ7AvhvlZKWdNx9/6I\nuJfWH8O7aH281RxRSrml4+4DEXEf8BBwGq2Pu3PBKuA1zN3vJSazZ3xv6txYSrm24+4DEbEFuCMi\nlpRSNs1kgzD7vph7DNhF64B5p0XAlplvp16llFFgIzBnVg1UtIXWsfx5sR8B2v/xPsYc2ZcRcTVw\nBnBaKeWRjoe2AC+JiAPHPWVO7btx43t0kvJ7aP29puy7WRXCpZRngfXA6Xu2tT9SnA78PKuvukTE\ny2h9lJ3sj2ROaQfSFsbuxwNpfWPdd/sRICKOAg5mDuzLdkCdBfxDKWXzuIfXA88xdt+dALwC+MWM\nNTkNk4xvb15P6/BZyr6bjYcjrgK+FRHrgXuBYeClwLcym+qFiPgS8B+0DkEcCfwLrT/42ffDV5OI\niAW0Zg7R3nR0RJwIbC+l/J7WsbhPRsRvaV0h77O0Vrn8IKHdrk00vvbtUlrHhLe0675I61PNmue/\n2uzRXg/bAM4EnoiIPZ9WRkspT5dSdkTEdcBVEfFn4C/AV4GflVLuzem6usnGFxFHA+8BbgMeB06k\nlTnrSin3Z/ScvjzjBZaVXETrP9ynaP3r+4bsnno0riatIHqK1rfNNwNLsvua4ljeTGvpz65xt+s7\nai6j9eXHk7TC6djsvnsxPlqXKVxNK4CfBv4X+Dfg0Oy+K4xrb2PaBby3o2Y/WmttH6MVwt8BDsvu\nvRfjA44C7gS2tf8uf0PrS9WXZfXspSwlKdGsOiYsSfONISxJiQxhSUpkCEtSIkNYkhIZwpKUyBCW\npESGsCQlMoQlKZEhLEmJDGFJSmQIS1Ki/wMMdshTdH9MXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e4c977160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os # para criar pastas\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# criamos uma pasta para salvar o modelo\n",
    "if not os.path.exists('tmp'): # se a pasta não existir\n",
    "    os.makedirs('tmp') # cria a pasta\n",
    "\n",
    "# baixa os dados em na pasta criada e carrega os dados \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"tmp/\", one_hot=False) # repare que não usamos vetores one-hot\n",
    "\n",
    "# mostra uma imagem\n",
    "plt.imshow(data.train.images[100].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Escopos de Nomes\n",
    "\n",
    "No TensorFlow, quando um nó é adicionada ao grafo, isso é feito sob um escopo. Nós podemos pensar nisso como o **nome que atribuímos ao nó dentro do grafo**. Esse nome não é necessariamente o mesmo que damos à variável no Python. Por exemplo, no grafo abaixo, definimos um nó com uma constante e atribuímos a ela o nome nome `b`, tanto no Python como no grafo TensorFlow. Podemos usar `print()` na variável definida em Python para confirmar o nome dela no grafo TensorFlow. Em seguida, nós redefinimos a constante `b` no Python, passando um novo nó com uma nova constante a ela. Também damos a esse nó o  nome `b`, no TensorFlow. No entanto, quando usamos `print()` para confirmar o nome desse nó, não é `b` que aparece, mas `b_0`. Por que isso acontece?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"b:0\", shape=(), dtype=int32)\n",
      "Tensor(\"b_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # criamos constante com valor 8\n",
    "    b = tf.constant(8, name='b')\n",
    "    print(b)\n",
    "    \n",
    "    # criamos constante com valor 0\n",
    "    b = tf.constant(0, name='b')\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mesmo que tenhamos redefinido a variável `b`, cada vez que atribuímos um nó TensorFlow a uma variável no Python, **adicionamos este nó ao grafo TensorFlow**. Ou seja, no Python, houve uma redefinição com sobrescrição de `b`, mas no TensorFlow ambos os nós continuam existindo e **não há sobrescrição**. Em vez disso, adiciona-se `_1` a frente de `b`  para evitar a sobrescrição. \n",
    "\n",
    "Agora que sabemos como os nomes das variáveis se comportam dentro do grafo TensorFlow, estamos prontos para aprender sobre os escopos dos nomes. Em poucas palavras, os escopos servem para organizar o nosso código TensorFlow (e para melhorar a visualização com o TensorBoard, como veremos mais para frente). Como recomendação de organização, devemos agrupar nós similares em um escopo próprio. Agora, não há uma definição clara do que seja essa similaridade. Na verdade, ela é muito variável e difere de modelo para modelo. Por exemplo, suponha que tenhamos uma rede neural com 3 camadas. Cada camada terá suas variáveis `W` e `b`, além das operações de multiplicação de matriz e não linearidade. Então, em vez de ficar criando nomes para cada um desses nós, nós podemos agrupá-los sob um escopo da camada e dar o mesmo nome para todos eles. Vejamos como isso é feito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_layer/labels:0\", shape=(?,), dtype=int64)\n",
      "Tensor(\"output_layer/linear_transformation:0\", shape=(?, 10), dtype=float32)\n",
      "<tf.Variable 'first_layer/Weights:0' shape=(784, 512) dtype=float32_ref>\n",
      "<tf.Variable 'first_layer/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'second_layer/Weights:0' shape=(512, 512) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# definindo constantes \n",
    "lr = 0.01 # taxa de aprendizado\n",
    "n_iter = 1000 # número de iterações de treino\n",
    "batch_size = 128 # qtd de imagens no mini-lote (para GDE)\n",
    "n_inputs = 28 * 28 # número de variáveis (pixeis)\n",
    "n_l1 = 512 # número de neurônios da primeira camada\n",
    "n_l2 = 512 # número de neurônios da segunda camada\n",
    "n_outputs = 10 # número classes (dígitos)\n",
    "\n",
    "graph = tf.Graph() # cria um grafo\n",
    "with graph.as_default(): # abre o grafo para que possamos colocar nós\n",
    "    \n",
    "    # Camadas de Inputs\n",
    "    with tf.name_scope('input_layer'): # escopo de nome da camada de entrada\n",
    "        x_input = tf.placeholder(tf.float32, [None, n_inputs], name='images')\n",
    "        y_input = tf.placeholder(tf.int64, [None], name='labels')\n",
    "\n",
    "    # Camada 1\n",
    "    with tf.name_scope('first_layer'): # escopo de nome da primeira camada\n",
    "        # variáveis da camada\n",
    "        W1 = tf.Variable(tf.truncated_normal([n_inputs, n_l1]), name='Weights')\n",
    "        b1 = tf.Variable(tf.zeros([n_l1]), name='bias')\n",
    "\n",
    "        l1 = tf.add(tf.matmul(x_input, W1), b1, name='linear_transformation')\n",
    "        l1 = tf.nn.relu(l1, name='relu')\n",
    "    \n",
    "    # Camada 2 \n",
    "    with tf.name_scope('second_layer'): # escopo de nome da segunda camada\n",
    "        # variáveis da camada\n",
    "        W2 = tf.Variable(tf.truncated_normal([n_l1, n_l2]), name='Weights')\n",
    "        b2 = tf.Variable(tf.zeros([n_l2]), name='bias')\n",
    "\n",
    "        l2 = tf.add(tf.matmul(l1, W2), b2, name='linear_transformation')\n",
    "        l2 = tf.nn.relu(l2, name='relu')\n",
    "    \n",
    "    # Camada de saída\n",
    "    with tf.name_scope('output_layer'): # escopo de nome da camada de saída\n",
    "        # variáveis da camada\n",
    "        Wo = tf.Variable(tf.truncated_normal([n_l2, n_outputs]), name='Weights')\n",
    "        bo = tf.Variable(tf.zeros([n_outputs]), name='bias')\n",
    "        \n",
    "        scores = tf.add(tf.matmul(l2, Wo), bo, name='linear_transformation') # logits\n",
    "        error = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_input, logits=scores),\n",
    "            name='error')\n",
    "        \n",
    "    # calcula acurácia\n",
    "    correct = tf.nn.in_top_k(scores, y_input, 1) # calcula obs corretas (vetor bools V ou F)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # converte de bool para float32\n",
    "        \n",
    "    # otimizador\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(error)\n",
    "\n",
    "    # inicializador\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # para salvar o modelo treinado\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # conferindo os nomes\n",
    "    print(y_input)\n",
    "    print(scores)\n",
    "    print(W1)\n",
    "    print(b1)\n",
    "    print(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Acima, agrupamos nosso modelo de rede neural em 4 camadas: uma de entrada, duas camadas ocultas de neurônios com ativação `ReLU` e uma camada de saída. Note como mudamos a estrutura de construção do grafo, relativa a que ensinei no [tutorial passado](https://matheusfacure.github.io/2017/05/12/tensorflow-essencial/). Lá, eu disse para começarmos definindo as variáveis de todas as camadas e só então montar o modelo. Aqui, definimos as variáveis junto com a montagem do modelo. Não há uma regra geral sobre qual forma de estruturar um programa TensorFlow é melhor. Eu costumo usar estrutura do tutorial passado na maioria dos casos (com definição de variáveis separada da montagem do modelo). No entanto, a forma acima faz mais sentido quando estamos organizando o código em escopos de nomes, o que geralmente é melhor com modelos muito grandes e complicados.\n",
    "\n",
    "Vamos agora, por partes, explicar o que fizemos acima. Como sempre, primeiro definimos algumas constantes, que serão os hiper-parâmetros do nosso modelo. Depois, passamos à fase de construção do grafo, começando com a montagem do modelo, o que fazemos por camadas. Abrimos um escopo para a camada de *inputs* com `with tf.name_scope():` e colocamos os nós de * inputs* - os *placeholders* - dentro desse escopo. Em seguida, abrimos o escopo da primeira camada oculta. Dentro dele, primeiro adicionamos as variáveis dessa camada e depois realizamos uma transformação linear, seguida de uma `ReLU`.  Criamos então a segunda camada oculta de forma idêntica à criação da primeira. Por fim, abrimos um escopo para a camada de saída. Nele, criamos nós para as variáveis da camada, para os *logits*, para a probabilidade prevista (`y_hat`) e para o erro. Note que a nossa camada de *output* retorna *logits* que são vetores de tamanho 10, com casa espaço representando a probabilidade prevista do dígito correspondente. No entanto, nossos alvos são variáveis categóricas no formato de um escalar, isto é, um único número. Por conta disso, usamos como custo a função com esse nome gigante `sparse_softmax_cross_entropy_with_logits()`. Ela faz três coisas: (1) converte os alvos de variáveis categóricas para vetores *one-hot*; (2) aplica a transformação softmax, que converte os *logits* em probabilidades (valores entre 0 e 1, somando 1); (3) calcula o custo de entropia cruzada.  \n",
    "\n",
    "Terminada a construção do nosso modelo, adicionamos nós para calcular a acurácia,  um nó com o otimizador para treinar o modelo, um nó para iniciar as variáveis e um nó para salvar o modelo treinado. Por fim, usamos `print()` para verificar o nome das variáveis no grafo TensorFlow. Note como o nome do escopo é adicionado antes do nome do nó. Vamos agora para a fase de execução desse grafo. Note como ela é extremamente similar a que fizemos no tutorial passado. A única diferença é a forma como a acurácia é calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro de treino na iteração 0: 1924.37\n",
      "Erro de validação na iteração 0: 1437.47\n",
      "Acurácia de validação na iteração 0: 0.21\n",
      "\n",
      "Erro de treino na iteração 1000: 6.81\n",
      "Erro de validação na iteração 1000: 8.84\n",
      "Acurácia de validação na iteração 1000: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# abrimos a sessão tf\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run() # iniciamos as variáveis\n",
    "    \n",
    "    # loop de treinamento\n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        # cria os mini-lotes\n",
    "        x_batch, y_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # cria um feed_dict\n",
    "        feed_dict = {x_input: x_batch, y_input: y_batch}\n",
    "\n",
    "        # executa uma iteração de treino e calcula o erro\n",
    "        l, _ = sess.run([error, optimizer], feed_dict=feed_dict)\n",
    "            \n",
    "        # mostra o progresso a cada 1000 iterações\n",
    "        if step % 1000 == 0:\n",
    "            \n",
    "            x_valid, y_valid = data.validation.next_batch(512) # pega alguns dados de validação\n",
    "            val_dict = {x_input: x_valid, y_input: y_valid} # monta o feed_dict\n",
    "            \n",
    "            # executa o nó para calcular a acurácia\n",
    "            error_np, acc = sess.run([error, accuracy], feed_dict=val_dict) \n",
    "            \n",
    "            print('Erro de treino na iteração %d: %.2f' % (step, l))\n",
    "            print('Erro de validação na iteração %d: %.2f' % (step, error_np))\n",
    "            print('Acurácia de validação na iteração %d: %.2f\\n' % (step, acc))\n",
    "\n",
    "            # salva as variáveis do modelo\n",
    "            saver.save(sess, \"./tmp/deep_ann.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Factoring out\n",
    "\n",
    "Você deve ter reparado que o código acima é bastante repetitivo. Seria bom se pudéssemos usar um pouco de modularidade, colocando algumas partes da construção do grafo em funções. Isso pode ser feito facilmente e torna o código muito mais fácil de manter e menos suscetível a error de copia e cola. Abaixo, nós criamos uma função que adiciona ao grafo as variáveis de uma camada de neurônios da rede neural. Ela aceita como argumento uma matriz de variáveis, que pode ser tanto a camada de *inputs* na nossa rede neural quanto os *outputs* de uma camada intermediária (lembre-se que uma camada da rede neural trata os *outputs* da camada anterior exatamente como a primeira camada trata os dados originais; afinal, o que a rede neural faz é aprender variáveis representativas ao longo das camadas). Em seguida, na função, descobrimos o formato dessa matriz de *input* com `inputs.get_shape()`. Nós pegamos o número de colunas da matriz de *inputs*, ou seja, o número de variáveis, e com isso mais o número de neurônios da camada, passado como argumento `n_neurons`, criamos as variáveis `W` e `b` da camada em questão. Por fim, nós realizamos a tranformação linear da camada e aplicamos a não linearidade, caso a função de ativação seja passada como argumento. \n",
    "\n",
    "O resto do código reimplementa o mesmo gráfico que vimos acima, mas agora usando essa função para criar as camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conected_layer(inputs, n_neurons, name_scope, activations=None):\n",
    "    '''Adiciona os nós de uma camada ao grafo TensorFlow'''\n",
    "    with tf.name_scope(name_scope):\n",
    "        \n",
    "        # define as variáveis da camada\n",
    "        n_inputs = int(inputs.get_shape()[1]) # pega o formato dos inputs\n",
    "        W = tf.Variable(tf.truncated_normal([n_inputs, n_neurons]), name='Weights')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='biases')\n",
    "        \n",
    "        # operação linar da camada\n",
    "        layer = tf.add(tf.matmul(inputs, W), b, name='linear_transformation')\n",
    "        \n",
    "        # aplica não linearidade, se for o caso\n",
    "        if activations == 'relu':\n",
    "            layer = tf.nn.relu(layer, name='relu')\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Camadas de Inputs\n",
    "    with tf.name_scope('input_layer'):\n",
    "        x_input = tf.placeholder(tf.float32, [None, n_inputs], name='images')\n",
    "        y_input = tf.placeholder(tf.int64, [None], name='labels')\n",
    "\n",
    "    # Camada 1\n",
    "    l1 = fully_conected_layer(x_input, n_neurons=n_l1, name_scope='first_layer', activations='relu')\n",
    "    \n",
    "    # Camada 2\n",
    "    l2 = fully_conected_layer(l1, n_neurons=n_l2, name_scope='second_layer', activations='relu')\n",
    "        \n",
    "    # Camada de saída\n",
    "    scores = fully_conected_layer(l2, n_neurons=n_outputs, name_scope='output_layer') # logits\n",
    "    error = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_input, logits=scores),\n",
    "        name='error')\n",
    "    \n",
    "    # calcula acurácia\n",
    "    correct = tf.nn.in_top_k(scores, y_input, 1) # calcula obs corretas\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # converta para float32\n",
    "    \n",
    "    # otimizador\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(error)\n",
    "\n",
    "    # inicializador\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # para salvar o modelo treinado\n",
    "    saver = tf.train.Saver()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "O que temos que ter em mente, é que a função que criamos **NÃO**  performa nenhuma computação. Ela apenas **adiciona os nós (variáveis e operações) de uma camada ao grafo TensorFlow**. Assim, quando chamamos `l1 = fully_conected_layer(...)` o que de fato fazemos é adicionar as variáveis da camada ao grafo. A execução do grafo construído acima é exatamente mesma dos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro de treino na iteração 0: 2094.33\n",
      "Erro de validação na iteração 0: 1727.43\n",
      "Acurácia de validação na iteração 0: 0.18\n",
      "\n",
      "Erro de treino na iteração 1000: 14.40\n",
      "Erro de validação na iteração 1000: 13.60\n",
      "Acurácia de validação na iteração 1000: 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# abrimos a sessão tf\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run() # iniciamos as variáveis\n",
    "    \n",
    "    # loop de treinamento\n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        # cria os mini-lotes\n",
    "        x_batch, y_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # cria um feed_dict\n",
    "        feed_dict = {x_input: x_batch, y_input: y_batch}\n",
    "\n",
    "        # executa uma iteração de treino e calcula o erro\n",
    "        l, _ = sess.run([error, optimizer], feed_dict=feed_dict)\n",
    "            \n",
    "        # mostra o progresso a cada 1000 iterações\n",
    "        if step % 1000 == 0:\n",
    "            \n",
    "            x_valid, y_valid = data.validation.next_batch(512) # pega alguns dados de validação\n",
    "            val_dict = {x_input: x_valid, y_input: y_valid} # monta o feed_dict\n",
    "            error_np, acc = sess.run([error, accuracy], feed_dict=val_dict) # calcula probabilidades e erro \n",
    "            \n",
    "            print('Erro de treino na iteração %d: %.2f' % (step, l))\n",
    "            print('Erro de validação na iteração %d: %.2f' % (step, error_np))\n",
    "            print('Acurácia de validação na iteração %d: %.2f\\n' % (step, acc))\n",
    "\n",
    "            # salva as variáveis do modelo\n",
    "            saver.save(sess, \"./tmp/deep_ann.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Vizualização\n",
    "\n",
    "Se você já está achando o TensorFlow uma ferramente fenomenal, agora terá certeza disso. Assim que você instala o TensorFlow, junto vem o [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard), uma ferramente de vizualização super completa, que permite analisar o grafo criado, resumir estatísticas de erro e ver como os parâmetros evoluem. Em se tratando de redes neurais, ver o grafo pode ser muito útil para entender o fluxo de dados no interior da rede. Além disso, ver como o treinamento evolui é a melhor técnica de *debugging* de redes neurais. Vamos então ver como trabalhar com o TensorBoard. Primeiro reformar nossa função de criar camadas de neurônios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conected_layer(inputs, n_neurons, name_scope, activations=None):\n",
    "    '''Adiciona os nós de uma camada ao grafo TensorFlow'''\n",
    "    \n",
    "    n_inputs = int(inputs.get_shape()[1]) # pega o formato dos inputs\n",
    "    with tf.name_scope(name_scope):\n",
    "        \n",
    "        # define as variáveis da camada\n",
    "        with tf.name_scope('Parameters'):\n",
    "            W = tf.Variable(tf.truncated_normal([n_inputs, n_neurons]), name='Weights')\n",
    "            b = tf.Variable(tf.zeros([n_neurons]), name='biases')\n",
    "            \n",
    "            tf.summary.histogram('Weights', W) # para registrar o valor dos W\n",
    "            tf.summary.histogram('biases', b) # para registrar o valor dos b\n",
    "        \n",
    "        # operação linar da camada\n",
    "        layer = tf.add(tf.matmul(inputs, W), b, name='Linear_transformation')\n",
    "        \n",
    "        # aplica não linearidade, se for o caso\n",
    "        if activations == 'relu':\n",
    "            layer = tf.nn.relu(layer, name='ReLU')\n",
    "        \n",
    "        # para registar a ativação na camada\n",
    "        tf.summary.histogram('activations', layer)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Repare que as únicas mudanças são adições de *summaries* (ou resumos). Primeiro, usando `tf.summary.histogram()`, criamos um nó para salvar histogramas dos parâmetros da camada. Depois, criamos um nó para registrar um histograma da ativação (*output*) da camada. Vamos agora adicionar mais alguns nós de resumo no resto do grafo. Repare também como colocamos mais operações dentro de `tf.name_scope()`. Isso é feito apenas por motivo estético e tornará a visualização do nosso grafo mais agrupada. Também usamos `tf.summary.scalar()` para resumir a evolução de uma variável representada por um único número (ou seja, um escalar), tais como o erro em termos de entropia cruzada e em termos de acurácia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdir = 'logs' # nome pasta para salvar os arquivos de visualização\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Camadas de Inputs\n",
    "    with tf.name_scope('input_layer'):\n",
    "        x_input = tf.placeholder(tf.float32, [None, n_inputs], name='images')\n",
    "        y_input = tf.placeholder(tf.int64, [None], name='labels')\n",
    "\n",
    "    # Camada 1\n",
    "    l1 = fully_conected_layer(x_input, n_neurons=n_l1, name_scope='First_layer', activations='relu')\n",
    "    \n",
    "    # Camada 2\n",
    "    l2 = fully_conected_layer(l1, n_neurons=n_l2, name_scope='Second_layer', activations='relu')\n",
    "        \n",
    "    # Camada de saída\n",
    "    scores = fully_conected_layer(l2, n_neurons=n_outputs, name_scope='Output_layer') # logits\n",
    "    \n",
    "    # camada de erro\n",
    "    with tf.name_scope('Error_layer'):\n",
    "        error = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_input, logits=scores),\n",
    "                               name='error')\n",
    "        tf.summary.scalar('Cross_entropy', error) # para registrar a função custo\n",
    "    \n",
    "    with tf.name_scope(\"Accuracy\"):\n",
    "        correct = tf.nn.in_top_k(scores, y_input, 1) # calcula obs corretas\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # converta para float32\n",
    "        tf.summary.scalar('Accuracy', accuracy) # para registrar a função custo\n",
    "        \n",
    "    # otimizador\n",
    "    with tf.name_scope('Train_operation'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(error)\n",
    "        \n",
    "    # inicializador\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # para salvar o modelo treinado\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # para registrar na visualização\n",
    "    summaries = tf.summary.merge_all() # funde todos os summaries em uma operação\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # para escrever arquivos summaries\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Para que não tenhamos que rodar todos os resumos individualmente na fase de execução do grafo, usamos `tf.summary.merge_all()` para fundir todos os nós de resumo em um só. Na fase de execução então, adicionamos o nó `summaries` às operações para serem executadas durante uma iteração de treino. Quando esse nó é executado, ele retorna uma *string* (texto) codificando a informação dos resumos naquela iteração de treino. Repare que como executamos os resumos durante o treino, a acurácia que será armazenada pelo TensorBoard será a de treino, referente ao mini-lote daquela iteração. Por fim, ainda na fase de construção, com `tf.summary.FileWriter()` criamos um nó que se encarregará de escrever a informação dos resumos no disco. Para a construção desse nó, passamos o nome de uma pasta (diretório) e passamos também o grafo que será salvo para visualização (nesse caso, o grafo que estamos construindo, que é acessado com `tf.get_default_graph()`).\n",
    "\n",
    "A cada 10 iterações, vamos escrever no disco a *string* de informações dos resumos. Fazemos isso com o método `file_writer.add_summary()` do nó que criamos antes para salvar informação no disco. Escrever no disco é uma operação demorada, principalmente se seu disco rígido for um HD e não um SSD. Isso significa que usar o TensorBoard aumenta drasticamente o tempo de treinamento de uma rede neural, devendo então ser utilizado apenas quando o objetivo é a visualização em si ou *debugging*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro de treino na iteração 0: 2471.73\n",
      "Erro de validação na iteração 0: 1733.50\n",
      "Acurácia de validação na iteração 0: 0.19\n",
      "\n",
      "Erro de treino na iteração 200: 39.70\n",
      "Erro de validação na iteração 200: 38.54\n",
      "Acurácia de validação na iteração 200: 0.92\n",
      "\n",
      "Erro de treino na iteração 400: 35.02\n",
      "Erro de validação na iteração 400: 23.41\n",
      "Acurácia de validação na iteração 400: 0.94\n",
      "\n",
      "Erro de treino na iteração 600: 5.25\n",
      "Erro de validação na iteração 600: 14.61\n",
      "Acurácia de validação na iteração 600: 0.95\n",
      "\n",
      "Erro de treino na iteração 800: 20.52\n",
      "Erro de validação na iteração 800: 9.58\n",
      "Acurácia de validação na iteração 800: 0.96\n",
      "\n",
      "Erro de treino na iteração 1000: 7.65\n",
      "Erro de validação na iteração 1000: 16.07\n",
      "Acurácia de validação na iteração 1000: 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# abrimos a sessão tf\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run() # iniciamos as variáveis\n",
    "    \n",
    "    # loop de treinamento\n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        # cria os mini-lotes\n",
    "        x_batch, y_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "        # cria um feed_dict\n",
    "        feed_dict = {x_input: x_batch, y_input: y_batch}\n",
    "\n",
    "        # executa uma iteração de treino e calcula o erro\n",
    "        l, summaries_str, _ = sess.run([error, summaries, optimizer], feed_dict=feed_dict)\n",
    "        \n",
    "        # a cada 10 iterações, salva os registros dos summaries\n",
    "        if step % 10 == 0:\n",
    "            file_writer.add_summary(summaries_str, step)\n",
    "        \n",
    "        # mostra o progresso a cada 1000 iterações\n",
    "        if step % 200 == 0:\n",
    "            \n",
    "            x_valid, y_valid = data.validation.next_batch(512) # pega alguns dados de validação\n",
    "            val_dict = {x_input: x_valid, y_input: y_valid} # monta o feed_dict\n",
    "            error_np, acc = sess.run([error, accuracy], feed_dict=val_dict) # calcula probabilidades e erro \n",
    "            \n",
    "            print('Erro de treino na iteração %d: %.2f' % (step, l))\n",
    "            print('Erro de validação na iteração %d: %.2f' % (step, error_np))\n",
    "            print('Acurácia de validação na iteração %d: %.2f\\n' % (step, acc))\n",
    "\n",
    "            # salva as variáveis do modelo\n",
    "            saver.save(sess, \"./tmp/deep_ann.ckpt\")\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Agora, na pasta onde você rodou esse código, deve haver uma pasta com o nome `logs`. Na linha de comando digite\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir logs/\n",
    "```\n",
    "\n",
    "Isso inicializará o TensorBoard. Vá em um Browser (Chrome ou Firefox) e navegue para http://localhost:6006 para proceder com a visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
