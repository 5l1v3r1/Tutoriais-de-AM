{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "- [Pré-requisitos](#pre_requisitos)\n",
    "- [Justificativa matemática](#justificativa_matematica)\n",
    "- [Desenhando e testando o algoritmo](#desenhando_e_testando_o_algoritmo)\n",
    "- [Indo um pouco além: Projeção com Erros Ortogonais](#indo_um_pouco_alem)\n",
    "- [Introduzindo relações não lineares](#introduzindo_relacoes_nao_lineares)\n",
    "- [Recomendações e Considerações Finais](#recomendacoes_e_consideracoes_finais)\n",
    "- [Ligações Externas](#ligacoes_externas)\n",
    "\n",
    "<br/>\n",
    "<a id='pre_requisitos'></a>\n",
    "## Pré-requisitos\n",
    "\n",
    "É preciso ter um conhecimento básico de Python, incluindo o mínimo de Python orientado à objetos. Caso não saiba programar, os cursos de [Introdução à Ciencia da Computação](https://br.udacity.com/course/intro-to-computer-science--cs101/) e [Fundamentos de Programação com Python](https://br.udacity.com/course/programming-foundations-with-python--ud036/) fornecem uma base suficiente sobre programação em Python e Python orientado à objetos, respectivamente. Além disso, é necessário ter conhecimento das bibliotecas de manipulação de dados Pandas e Numpy. Alguns bons tutoriais são o [Mini-curso 1](https://br.udacity.com/course/machine-learning-for-trading--ud501/) do curso de Aprendizado de Máquina para Negociação, o site [pythonprogramming.net](https://pythonprogramming.net/data-analysis-python-pandas-tutorial-introduction/) ou o primeiro curso do DataCamp em [Python](https://www.datacamp.com/getting-started?step=2&track=python).\n",
    "\n",
    "Para entender o desenvolvimento do algoritmo de regressão linear é preciso ter o conhecimento de introdução à álgebra linear. Na UnB, a primeira parte do curso de Economia Quantitativa 1 já cobre o conteúdo necessário. Caso queira relembrar ou aprender esse conteúdo, o curso online do MIT de [Introdução à Álgebra Linear](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8) fornece uma boa base sobre a matemática que será desenvolvida nos algoritmos de aprendizado de máquina.\n",
    "\n",
    "Conhecimento de cálculo e principalmente otimização é fundamental para o entendimento dos algoritmos de aprendizado de máquina, que muitas vezes são encarados explicitamente como problemas de otimização. Uma noção de cálculo multivariado também ajudará na compreensão dos algoritmos, visto que muitas vezes otimizaremos em várias direções.\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "<a id='justificativa_matematica'></a>\n",
    "## Justificativa matemática\n",
    "\n",
    "Imagine que temos dados em tabelas, sendo que cada linha é uma observação e cada coluna uma variável. Nos então escolhemos uma das colunas para ser nossa variável dependente y (aquela que queremos prever) e as outras serão as variáveis independentes (X). Nosso objetivo é aprender como chegar das variáveis independentes na variável dependente, ou, em outras palavras, prever y a partir de X. Note, que X é uma matriz nxd, em que n é o número de observações e d o número de dimensões; y é um vetor coluna nx1. Podemos definir o problema como um sistema de equações, em que cada equação é uma observação:\n",
    "\n",
    "$\\begin{cases} \n",
    "w_0 + w_1 x_1 + ... + w_d x_1 = y_1 \\\\\n",
    "w_0 + w_1 x_2 + ... + w_d x_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_n + ... + w_d x_n = y_n \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Normalmente, $n > d$, isto é, temos mais observações que dimensões. Sistemas assim costumam não ter solução; há muitas equações e poucas variáveis para ajustar. Intuitivamente, pese que, na prática, muitas coisas afetam a variável y. Principalmente se ela for algo de interesse das ciências humanas, como, por exemplo, preço, desemprego, felicidade... E muitas das coisas que afetam y não podem ser coletadas como dados; as equações acima não tem solução porque não temos todos os fatores que afetam y. \n",
    "\n",
    "Para lidar com esse problema, vamos adicionar nas equações um termo erro $\\varepsilon$ que representará os fatores que não conseguimos observar, erros de medição, etc.\n",
    "\n",
    "$$\\begin{cases} \n",
    "w_0 + w_1 x_{11} + ... + w_d x_{1d} +  \\varepsilon_1 = y_1 \\\\\n",
    "w_0 + w_1 x_{21} + ... + w_d x_{2d} + \\varepsilon_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_{n1} + ... + w_d x_{nd} + \\varepsilon_3 = y_n \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Em forma de matriz:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "    1 & x_{11} & ... & x_{1d} \\\\\n",
    "    1 & x_{21} & ... & x_{2d} \\\\\n",
    "    \\vdots &  \\vdots&  \\vdots &  \\vdots \\\\\n",
    "    1 & x_{n1} & ... & x_{nd} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "    w_0 \\\\\n",
    "    w_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_d \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    \\varepsilon_0 \\\\\n",
    "    \\varepsilon_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\varepsilon_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    y_0 \\\\\n",
    "    y_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$X_{nd} \\pmb{w}_{d1} + \\pmb{\\epsilon}_{n1} = \\pmb{y}_{n1}$$\n",
    "\n",
    "\n",
    "\n",
    "Para estimar a equação acima, usaremos a técnica de Mínimos Quadrados Ordinários (MQO): queremos achar os $\\pmb{\\hat{w}}$ que minimizam os $n$ $ \\varepsilon^2 $, ou, na forma de vetor, $\\pmb{\\epsilon}^T \\pmb{\\epsilon}$. Por que minimizar os erros quadrados? Bom, não há uma resposta certa para isso. Note que os erros variam para mais e para menos e tem média zero, de forma que a soma deles será sempre muito próxima de zero. Então temos que fazer algo para que todos os erros sejam positivos. Poderíamos minimizar os erros absolutos, mas a o quadrado dos erros também funciona e deixa a matemática bem mais simples: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\pmb{\\epsilon}^T  \\pmb{\\epsilon} &= (\\pmb{y} - \\pmb{\\hat{w}}X)^T(\\pmb{y} - \\pmb{\\hat{w}} X) \\\\\n",
    "             &= \\pmb{y}^T \\pmb{y} - \\pmb{\\hat{w}}^T X^T \\pmb{y} - \\pmb{y}^T X \\pmb{\\hat{w}} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}} \\\\\n",
    "             &= \\pmb{y}^T \\pmb{y} - 2\\pmb{\\hat{w}}^T X^T \\pmb{y} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Aqui, usamos o fato que que $\\pmb{\\hat{w}}^T X^T \\pmb{y}$ e $\\pmb{y}^T X \\pmb{\\hat{w}}$ são simplesmente escalares $1x1$ e a transposta de um escalar é o mesmo escalar: $\\pmb{\\hat{w}}^T X^T \\pmb{y} = (\\pmb{\\hat{w}}^T X^T \\pmb{y})^T = \\pmb{y}^T X \\pmb{\\hat{w}}$. Derivando em $\\pmb{\\hat{w}}$ e achando a CPO:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\pmb{\\epsilon}^T \\pmb{\\epsilon}}{\\partial \\pmb{\\hat{w}}} = -2X^T\\pmb{y} + 2X^T X \\pmb{\\hat{w}} = 0$$\n",
    "\n",
    "\n",
    "Derivando mais uma vez para checar a CSO chegamos em $2X^TX$, que é positiva definida se as colunas de X forem independentes. Temos então um ponto de mínimo quando:\n",
    "\n",
    "\n",
    "$$ \\pmb{\\hat{w}} = (X^T X)^{-1} X^T \\pmb{y}$$\n",
    "\n",
    "\n",
    "Bom, parece que chegamos em algo interessante. Nos nossos dados temos $X$ e $\\pmb{y}$, então podemos achar $\\hat{\\pmb{w}}$ facilmente: basta substituir os valores na fórmula! O próximo passo e desenhar o algoritmo e ver como ele se sai em dados reais.\n",
    "\n",
    "OBS:  \n",
    "1) Para mais detalhes, veja [este](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) passo a passo da Universidade de Stanford.  \n",
    "2) Seria possível chegar em uma fórmula para os vários $\\hat{w_i}$ apenas com cálculo multivariado, sem usar álgebra linear. Embora a forma com álgebra linear seja mais difícil (pelo menos foi para mim) ela vai nos ajudar no entendimento de como o algoritmo funciona. Álgebra linear é uma ferramente poderosa de abstração e a vasta maioria dos algoritmos de aprendizado de máquina usam álgebra linear em suas derivações, então é bom já irmos nos acostumando. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<a id='desenhando_e_testando_o_algoritmo'></a>\n",
    "## Desenhando e testando o algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # para ler os dados em tabela\n",
    "import numpy as np # para álgebra linear\n",
    "from sklearn import linear_model, model_selection, datasets # para comparar o nosso algoritmo com o de mercado\n",
    "import matplotlib.pyplot as plt # para fazer gráficos\n",
    "from matplotlib import style\n",
    "from time import time # para ver quanto tempo demora\n",
    "style.use('ggplot') # para gráficos bonitinhos\n",
    "np.random.seed(1) # para resultados consistentes \n",
    "\n",
    "class linear_regr(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # adiciona coluna de 1 nos dados\n",
    "        X = np.insert(X_train, 0, 1, 1)\n",
    "\n",
    "        # estima os w_hat\n",
    "        w_hat = np.dot( np.dot( np.linalg.inv(np.dot(X.T, X)), X.T), y_train)\n",
    "                                    # (X^T * X)^-1 * X^T * y\n",
    "        self.w_hat = w_hat\n",
    "        self.coef = self.w_hat[1:]\n",
    "        self.intercept = self.w_hat[0]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = np.insert(X_test, 0, 1, 1) # adiciona coluna de 1 nos dados\n",
    "        y_pred = np.dot(X, self.w_hat) # X * w_hat = y_hat\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, teoria justificada e algoritmo pronto. Vamos ver se ele consegue aprender os $\\hat{\\pmb{w}}$ de dados reais.\n",
    "OBS: Os dados podem ser encontrados em http://www.cengage.com/aise/economics/wooldridge_3e_datasets/.  \n",
    "  \n",
    "  \n",
    "Lendo e processando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>assess</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>lotsize</th>\n",
       "      <th>sqrft</th>\n",
       "      <th>colonial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>349.1</td>\n",
       "      <td>4</td>\n",
       "      <td>6126</td>\n",
       "      <td>2438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370.0</td>\n",
       "      <td>351.5</td>\n",
       "      <td>3</td>\n",
       "      <td>9903</td>\n",
       "      <td>2076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.0</td>\n",
       "      <td>217.7</td>\n",
       "      <td>3</td>\n",
       "      <td>5200</td>\n",
       "      <td>1374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195.0</td>\n",
       "      <td>231.8</td>\n",
       "      <td>3</td>\n",
       "      <td>4600</td>\n",
       "      <td>1448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373.0</td>\n",
       "      <td>319.1</td>\n",
       "      <td>4</td>\n",
       "      <td>6095</td>\n",
       "      <td>2514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  assess  bdrms  lotsize  sqrft  colonial\n",
       "0  300.0   349.1      4     6126   2438         1\n",
       "1  370.0   351.5      3     9903   2076         1\n",
       "2  191.0   217.7      3     5200   1374         0\n",
       "3  195.0   231.8      3     4600   1448         1\n",
       "4  373.0   319.1      4     6095   2514         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/hprice.csv', sep=',').ix[:, :6] # lendo os dados\n",
    "data.fillna(-99999, inplace = True) # preenchendo valores vazios\n",
    "X = np.array(data.drop(['price'], 1)) # escolhendo as variáveis independentes\n",
    "y = np.array(data['price']) # escolhendo a variável dependente\n",
    "\n",
    "# separa em bases de treino e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando, testando e comparando o regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo do criado manualmente: 0.001 s\n",
      "Média do erro absoluto:  34.9234990043\n",
      "Média do erro relativo:  0.122915533711\n",
      "\n",
      "\n",
      "Tempo do de mercado: 0.002 s\n",
      "Média do erro absoluto:  34.9234990043\n",
      "Média do erro relativo:  0.122915533711\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "regr = linear_regr()\n",
    "regr.fit(X_train, y_train)\n",
    "print(\"Tempo do criado manualmente:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "# medindo os erros\n",
    "y_hat = regr.predict(X_test) # prevendo os preços\n",
    "\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat - y_test)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat - y_test) / y_test)).mean())\n",
    "\n",
    "# comparando com o de mercado\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print(\"\\n\\nTempo do de mercado:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "# medindo os erros\n",
    "y_hat = regr.predict(X_test) # prevendo os preços\n",
    "w_hat = regr.intercept_\n",
    "w_hat = np.append(w_hat, regr.coef_)\n",
    "\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat - y_test)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat - y_test) / y_test)).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nada mal... O erro previsto é, na média, apenas 12,2% diferente do preço real/observado. Note que o algoritmo aprendeu os parâmetros $\\hat{\\pmb{w}}$ com uma parte dos dados e usou para prever dados que nunca tinha visto, mostando uma boa capacidade de generalização.\n",
    "\n",
    "O nosso algoritmo produz os mesmos resultados do de mercado, então podemos saber que não erramos nada. Além disso, o nosso algoritmo é mais rápido que o de mercado, mas essa diferênça é insignificante, em termos práticos. Cabe aqui uma observação: **não reinvente a roda!**. Na prática, se existe um bom algorítmo já feito, use-o! Não é preciso fazer o algorítmo do zero sempre, basta importar o do [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)! Aqui, estamos recriando os algorítmos apenas para melhor entendimento de como ele funciona, mas não com intenção de usar nossa criação na prática. Além disso, os algorítmos já pronto são muuuito melhores e mais rápidos que o nosso. O modelo de regressão linear é apenas uma exceção devido à sua simplicidade\n",
    "\n",
    "\n",
    "A vantágem do modelo de regressão linear é o que chamamos de um modelo caixa branca: nos sabemos exatamente como ele aprende os parâmetros e ainda nos oferece capacidade interpretativa por meios deles. Infelizmente, a capacidade interpretativa depende de um aprofundamento que não é a intenção desse tutorial. Caso queira se aprofundar no alorítmo, veja [1](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) ou [2](https://www.coursera.org/learn/erasmus-econometrics).\n",
    "\n",
    "Outra vantágem da regressão linear por MQO é que o processo de treinamento é muuuuuito rápido treinar. Muito mesmo. Mesmo com milhões de dados, é possível estimar os parâmetros em menos de um segundo. Além disso, uma vez treinado, o regressor ocupa muito pouco espaço, pois só armazena o vetor $\\pmb{\\hat{w}}$.\n",
    "\n",
    "Vale uma nota de atenção: esse algoritmo é a base da econometria e da ciência de dados inferencial no geral. Aqui só podemos abordá-lo brevemente. Ainda há problemas de iferência (saber se os coeficientes são estatisticamente significantes), de interpretação em outras escalar, de hipóteses assumidas e o que fazer quando elas são violadas. Tenha isso em mente na hora de usá-lo! Muita coisa ficou incompleta aqui. Caso tenha interesse em se aprofundar no assunto, apontaremos fontes externas para isso no final do tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06d334ad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wBA6XWrSPtPFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06cf80cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Código para os gráficos:\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-15,15, 20)\n",
    "y = x * .5 + 2 + np.random.normal(0,2,size=20)\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(1, 1 * .5 + 2, c='C4', s=100) # x=1\n",
    "plt.plot(x, x * .5 + 2, c='C1')\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<a id='indo_um_pouco_alem'></a>   \n",
    "## Indo um pouco além: Projeção com Erros Ortogonais\n",
    "\n",
    "Nós chegamos em um fórmula muito útil $\\pmb{\\hat{w}} = (X^T X)^{-1} X^T \\pmb{y}$, mas, para mim, ainda não está claro o que essa fórmula faz, além de minimizar os erros quadrados. O objetivo aqui é entender melhor como o algorítmo funciona por meio de vizualização e exemplos. \n",
    "\n",
    "Bom, a primeira coisa que notamos é que $X\\pmb{\\hat{w}}$ produz $\\pmb{\\hat{y}}$ e não $\\pmb{y}$. Há uma diferênça entre $\\pmb{\\hat{y}}$ e $\\pmb{y}$ que é um resíduo $\\pmb{\\epsilon}$. Podemos então definir:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\pmb{\\epsilon} &= \\pmb{y} - X \\pmb{\\hat{w}} \\\\\n",
    "             &= \\pmb{y} - X (X^T X)^{-1} X^T \\pmb{y} \\\\\n",
    "             &= [I -  X (X^T X)^{-1} X^T] \\pmb{y} \\\\\n",
    "             &= M\\pmb{y}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Além disso:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\pmb{\\hat{y}} &= \\pmb{y} - \\pmb{e} \\\\\n",
    "             &= [I - M]\\pmb{y} \\\\\n",
    "             &= X (X^T X)^{-1} X^T \\pmb{y}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Chamaremos $X (X^T X)^{-1} X^T$ de $P$. A matriz $P$ transforma $\\pmb{y}$ em $\\pmb{\\hat{y}}$, mas como? De alguma forma, eu acredito que entender essa matriz é a chave para vizualizar como o algorítmo de MQO funciona. Vamos criar um exemplo hipotético com poucos números para facilitar a vizualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2],\n",
    "              [2, 1],\n",
    "              [3, 4],\n",
    "              [5, 1],\n",
    "              [2, 6],\n",
    "              [3, 3]])\n",
    "w = np.array([[4], \n",
    "              [1]])\n",
    "\n",
    "y =  np.dot(X, w) + np.reshape(np.random.normal(0, 0.5, 6), (6, 1))\n",
    "\n",
    "np.round(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.dot(np.dot(X, np.linalg.inv(np.dot(X.T, X))), X.T)\n",
    "y_hat = np.dot(P, y)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note como $\\pmb{\\hat{y}}$ representa a relação linear entre $X$ e $y$ retirando parte do ruido. Por exemplo, para a primeira observação, não fosse o ruido, $y_1$ seria $6$ e $\\hat{y_1}$ é bem mais próximo de $6$ do que $y_1$. Se todas as variáveis relevantes estão em $X$, a matrix $P$ pode então ser vista como um filtro de ruido. A forma como eu gosto de vizualizar $P$ é como uma matriz projeção: $P$ projeta o vetor $\\pmb{y}$ em $\\pmb{\\hat{y}}$ de forma que o reíduo seja ortogonal a $\\pmb{\\hat{y}}$. O resíduo ser ortogonal significa que ele não tem nenhuma relação com $\\pmb{\\hat{y}}$ e se ele também não tiver correlação com $\\pmb{y}$, então estaremos achando a melhor relação entre as variável dependentes e independentes.\n",
    "\n",
    "Mas isso chama a atenção para um problema muito sério: normalmente não temos todas as variáveis relevantes para explicar um fenômeno. O que acontece nesse caso?\n",
    "\n",
    "Vamos refazer o exemplo, mas agora, suponha que não possâmos medir a segunda variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.delete(X, 1, 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, $x$ é apenas um vetor e $X (X^T X)^{-1} X^T$ se reduz para $\\frac{1}{k} X X^T$ em que $k = X^T X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.dot(x, x.T) / np.dot(x.T, x)\n",
    "y_hat = np.dot(P, y)\n",
    "y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que obtemos agora um resultado divergente do que seria o real sem o ruído. Para a primeira observação, por exemplo, sem ruído teriamos 6, mas a nossa estimativa é menor do que 4! Note que a variável que mais contribui para $y$ é $x_1$ (contribui 4x mais), e mesmo que a tenhamos, simplemente não conseguimos achar o resultado correto se nos falta $x_2$. Lembre que $P$ projeta o vetor $\\pmb{y}$ em $\\pmb{\\hat{y}}$ de forma que os resíduos sejam ortogonais, ou seja, de forma que eles não tenham nenhuma relação com $\\pmb{\\hat{y}}$. Isso ainda é verdade, mas agora o resíduo tem relação com $\\pmb{y}$! Isso acontece porque $\\pmb{\\epsilon}$ incorpora tudo o que não conseguimos observar e nesse caso não podemos observar uma das variáveis que afeta $\\pmb{y}$. Por isso, assumir que $\\pmb{y}$ é não depende de $\\pmb{\\epsilon}$ uma hipótese falha e o nosso erro de previsão sobre bastante.\n",
    "\n",
    "No entanto, essa é ainda a melhor estimativa que podemos fazer com os dados que temos (lembre que assumimos que não podiamos observar $x_2$). Como sabemos disso? Bom, ainda estamos minimizando os erros quadrados, esse é um argumento. Do ponto de vista de projeções, nós ainda estamos projetando $y$ em $\\hat{y}$ de forma que os resíduos sejam ortogonais à $\\hat{y}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = y - y_hat\n",
    "np.round(np.dot(e.T, y_hat), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viu? O produto interno entre dois vetores ortogonais é zero, então é de se esperar que tenhamos conseguido zero no produto interno acima: $\\pmb{\\epsilon}$ e $\\pmb{y}$ são ortogonais! Como estamos em apenas duas dimenções, podemos vizualizar as projeções facilmente. Lembre-se que, do ponto de vista geométrico, ortogonalidade se reflete em vetores perpendiculares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x,y_hat, color='k')\n",
    "plt.scatter(x, y)\n",
    "for i, xi in enumerate(list(x)):\n",
    "    plt.plot([xi, xi], [y[i], y_hat[i]], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas espere um minuto, isso não parece nada ortogonal... A linha vermelha não está paralela à preta! Eu quebrei a cabeça por horas com esse problema e ainda acho ele um tanto difícil então preste atenção e mostraremos como essas linhas são sim ortogonais.\n",
    "\n",
    "Primeiro, lembre que a linha preta é a equação $\\pmb{\\hat{y}} = \\pmb{x}\\pmb{\\hat{w}}$. A unica variável que influencia $\\pmb{\\hat{y}}$ é, nesse caso, $\\pmb{x}$. Pos isso, fizemos o gráfico acima em apenas duas dimenções: $x$ e $y$. No entanto, o gráfico acima também tem $\\pmb{y}$ (os pontos azuis). Mas $\\pmb{y}$, além de depender de $\\pmb{x}$ também depende de $\\pmb{\\epsilon}$: $\\pmb{y} = \\pmb{x}\\pmb{\\hat{w}} + \\pmb{\\epsilon}$. Há então uma dimenção faltando no gráfico acima se queremos vizualizar a relação entre $\\pmb{\\epsilon}$, $\\pmb{x}$, $\\pmb{\\hat{y}}$ e $\\pmb{y}$!\n",
    "\n",
    "Assim, quando adicionamos a dimenção relativa à  $\\pmb{\\epsilon}$ conseguimos ver como os resíduos são ortogonais ao plano definido por $\\pmb{\\hat{y}} = \\pmb{x}\\pmb{\\hat{w}}$. Agora sim, podemos vizualizar o que a matemática já tinha nos mostrado apenas com números!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "w_hat = np.dot( np.dot( np.linalg.inv(np.dot(x.T, x)), x.T), y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "X, E = np.meshgrid(x, e)\n",
    "Y_hat = X*w_hat + 0\n",
    "surf = ax.plot_surface(X, E, Y_hat, rstride=1, cstride=1, color='b')\n",
    "\n",
    "ax.scatter(x, e, y_hat, c='k', marker='o')\n",
    "ax.scatter(x, e, y, c='g', marker='o', s = 50)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<a id='introduzindo_relacoes_nao_lineares'></a>   \n",
    "## Introduzindo relações não lineares\n",
    "\n",
    "Até agora modelamos a relação entre $X$ e $y$ de forma linear. No entanto, a maioria das relações no mundo real **não** segue uma dinâmica linear! Pense no preço da casa, por exemplo. Pode ser que o aumento de preço de uma casa de 2 quartos para uma casa com 3 quartos seja diferente do aumento de preço de uma casa de 6 para 7 quartos! Uma relação linear não captura preços marginais decrescentes, por exemplo. Nos podemos facilmente introduzir não linearidade no modelo transformando as variáveis X, com uma transformação polinomial ou logarítmica, por exemplo. Vamos focar na transformação polinomial.\n",
    "\n",
    "Até agora, estávamos modelando um polinômio de grau 1. Podemos elevar cada variável ao quadrado subir o grau do polinômio:\n",
    "\n",
    "$$\\begin{cases} \n",
    "w_0 + w_1 x_{11} + ...  + w_d x_{1d} +  w_1 x^2_{11} + ...  + w_d x^2_{1d} + \\varepsilon_1 = y_1 \\\\\n",
    "w_0 + w_1 x_{21} + ...  + w_d x_{2d} +  w_1 x^2_{21} + ...  + w_d x^2_{2d} + \\varepsilon_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_{n1} + ...  + w_d x_{nd} +  w_1 x^2_{n1} + ...  + w_d x^2_{nd} + \\varepsilon_n = y_n \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Na forma de matriz, nada muda. Só devemos lembrar que $X$ nesse caso é diferente dos dados originais, porque cada variável virou duas, uma elevada a 1 e outra a 2. Aqui, vamos usar $X^*$ para explicitar essa diferença.\n",
    "\n",
    "$$X^* \\pmb{w}^* + \\pmb{\\epsilon} = \\pmb{y}$$\n",
    "\n",
    "Explicitando:\n",
    "\n",
    "$$X^* \\pmb{w}^* + \\pmb{\\epsilon} = \\pmb{y}$$\n",
    "$$X_{nd} \\pmb{w}_{d1} + X_{nd}^*  \\pmb{w}^*_{d1} +  \\pmb{\\epsilon} = \\pmb{y}$$\n",
    "\n",
    "Em que\n",
    "\n",
    "$\\pmb{w}^*  = \\begin{bmatrix}\n",
    "    x_1 w_{0+d} \\\\\n",
    "    x_2 w_{1+d} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_d w_{d+d} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Nosso algoritmo não muda nada. A única diferença é que os nossos inputs devem ser transformados para adicionar os componentes quadráticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(data.drop(['price'], 1))\n",
    "y = np.array(data['price'])\n",
    "\n",
    "# adicionando os componentes quadráticos\n",
    "X_star = X ** 2\n",
    "X = np.concatenate((X, X_star), axis=1)[:, :8]\n",
    "\n",
    "# separa em treino e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "regr = linear_regr()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# medindo os erros\n",
    "y_hat_in = regr.predict(X_train) # na amostra\n",
    "y_hat_out = regr.predict(X_test) # fora da amostra\n",
    "\n",
    "print('Na amostra')\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat_in - y_train)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat_in - y_train) / y_train)).mean())\n",
    "\n",
    "print('\\nFora da Amostra')\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat_out - y_test)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat_out - y_test) / y_test)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, comparado ao modelo de apenas um grau, nossa regressão no polinômio de grau 2 tem erro absoluto médio 2 pontos menor; erro relativo também caiu 1%. \n",
    "\n",
    "Algumas observações:  \n",
    "1) Temos nos dados uma variável binária (0,1) e quando fazemos o quadrado disso, obtemos a mesma variável. Ter uma variável repetida nos dados é um problema, pois uma uma coluna de $X$ é combinação linear da outra, fazendo com que $X^T X$ não seja inversível. Para resolver esse problema, retiramos a cópia da variável binária após elevar os dados ao quadrado.  \n",
    "2) Nem sempre aumentar o grau do polinômio diminuirá os erros. Polinômios de grau muito alto terão problemas de superajustamento, com erros na amostra muito baixos, mas erros fora da amostra bem altos, indicando baixa capacidade de generalização. Veja abaixo o que ocorre quando utilizamos um polinômio de grau 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(data.drop(['price'], 1))\n",
    "y = np.array(data['price'])\n",
    "\n",
    "# adicionando os componentes de maior grau\n",
    "X_star2 = X[:, :4] ** 2\n",
    "X_star3 = X[:, :4] ** 3\n",
    "X_star4 = X[:, :4] ** 4\n",
    "X_star5 = X[:, :4] ** 5\n",
    "\n",
    "X = np.concatenate((X, X_star2), axis=1)\n",
    "X = np.concatenate((X, X_star3), axis=1)\n",
    "X = np.concatenate((X, X_star4), axis=1)\n",
    "X = np.concatenate((X, X_star5), axis=1)\n",
    "\n",
    "\n",
    "# separa em treino e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "regr = linear_regr()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# medindo os erros\n",
    "y_hat_in = regr.predict(X_train) # na amostra\n",
    "y_hat_out = regr.predict(X_test) # fora da amostra\n",
    "\n",
    "print('Na amostra')\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat_in - y_train)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat_in - y_train) / y_train)).mean())\n",
    "\n",
    "print('\\nFora da amostra')\n",
    "print('Média do erro absoluto: ', np.absolute((y_hat_out - y_test)).mean())\n",
    "print('Média do erro relativo: ', np.absolute(((y_hat_out - y_test) / y_test)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<a id='recomendacoes_e_consideracoes_finais'></a>   \n",
    "## Recomendações e Considerações Finais\n",
    "O modelo de regressão linear acha um hiperplano linear que minimiza a soma dos resíduos quadrados entre a variável dependente (saída) prevista pelo hiperplano e a variável dependente observada. Para estimar os parâmetros $\\pmb{w}$ do hiperplano é necessário que nenhuma das variáveis independentes (entradas) seja combinação linear de outras. A não linearidade pode ser facilmente introduzida no modelo por meio de transformações diretamente nas variáveis dependentes. Essas transformações podem ser logaritmo de uma variável, quadrado de uma variável, etc.\n",
    "\n",
    "Em problemas de aprendizado de máquina, é recomendável utilizar o modelo de regressão linear como uma primeira tentativa, devido à sua simplicidade e capacidade interpretativa. Das vantagens do modelo de regressão linear podemos destacar:\n",
    "\n",
    "1) É de fácil interpretação. A variável de saída $y$ pode ser vista como uma soma das variáveis de entrada $X$ ponderada pelos parâmetros $w$. Assim, é possível saber diretamente qual variável de $X$ mais contribui para a variável de saída: a correspondente ao parâmetros com maior valor absoluto.  \n",
    "2) É um modelo rápido de treinar, uma vez que o ponto de minimização dos erros quadrados pode ser encontrada analiticamente, sem necessidade de métodos iterativos.  \n",
    "3) Uma vez treinado, o regressor ocupa puco espaço de RAM.  \n",
    "4) Produz bons resultados preditivos. Normalmente, o erro do modelo linear é apenas um pouco mais alto do que os obtidos com algoritmos de aprendizado de máquina mais complexo.  \n",
    "\n",
    "No entanto, o modelo vem com sérias desvantagens, o que nos motivará a procurar outros algoritmos mais complexos e melhores:\n",
    "\n",
    "1) A não linearidade tem que ser incluída à mão. O modelo não aprende a forma que se dão as relações não lineares.  \n",
    "2) Não é robusto à outliers. Não representa bem certas estruturas de dado, como pode ser visto no [Quarteto de Anscombe](https://pt.wikipedia.org/wiki/Quarteto_de_Anscombe), podendo produzir resultados enganosos.  \n",
    "3) Se duas variáveis de $X$ são altamente correlacionadas, o modelo vai ser sensível ao ruído. Recomenda-se utilizar Análise de Componentes Principais nas variáveis altamente correlacionadas e alimentar o modelo linear apenas com o primeiro componente principal.  \n",
    "\n",
    "<br/>\n",
    "<a id='ligacoes_externas'></a>   \n",
    "## Ligações Externas\n",
    "\n",
    "1) Para colocar a mão na massa:  \n",
    "O módulo do sklearn oferece boas implementações do algoritmo de regressão [linear](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) e [polinomial](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions).  \n",
    "\n",
    "<br/>\n",
    "2) Para entender mais:  \n",
    "O item sobre regressão no capítulo 2 do livro [Introduction to Machine Learning](https://mitpress.mit.edu/books/introduction-machine-learning) mostra bem o dilema entre superajustamento e subajustamento. A universidade de Stanford também tem um ótimo [documento](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) sobre regressão linear, muito mais aprofundado do que o tratado aqui.  \n",
    "Se você não entendeu muito bem o modelo de regressão linear eu aconselho procurar vários tutoriais e ver cada um deles até entender. Regressão linear é a pedra fundamental sobre a qual os outros algorítmos de aprendizado de máquina serão construidos. Seja diferindo, seja melhorando o modelo de regressão linear, os algorítmos mais complexos sempre tem alguma forma de comparação com o modelo visto aqui. Segue então uma lista de videos explicando regressão linear:  \n",
    "https://www.youtube.com/watch?v=D8PNnttuGZk&index=36&list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo  \n",
    "https://www.youtube.com/watch?v=udJvijJvs1M&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH&index=195  \n",
    "https://www.youtube.com/watch?v=kJvASBvZpw0&list=PLAwxTw4SYaPnVUrK_vL3r9tP6kuwAEzgQ&index=465  \n",
    "\n",
    "\n",
    "<br/>\n",
    "3) Para ir além:  \n",
    "O TensorFlow tem um ótimo [tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/linear/overview.html#what-is-a-linear-model) de como construir modelos lineares. Ao longo dele, faz diversas comparações com as redes neurais, que podem ser derivadas com pouca complexidade a partir dos modelos lineares.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
